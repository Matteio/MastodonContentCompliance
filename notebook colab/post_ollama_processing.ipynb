{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyML1DjynvUMn9A08Tx/vxFB","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8819968,"sourceType":"datasetVersion","datasetId":5306056}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/NicolaGabriele/MastodonContentCompliance/blob/main/post_ollama_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import requests\nimport json\nimport os\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:29:06.886823Z","iopub.execute_input":"2024-06-30T16:29:06.887198Z","iopub.status.idle":"2024-06-30T16:29:06.969877Z","shell.execute_reply.started":"2024-06-30T16:29:06.887169Z","shell.execute_reply":"2024-06-30T16:29:06.969181Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkDuXLuzUk1K","outputId":"09da6314-1017-4217-c391-6ee81bdeca7d","scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T16:32:26.481903Z","iopub.execute_input":"2024-06-30T16:32:26.482848Z","iopub.status.idle":"2024-06-30T16:32:29.570196Z","shell.execute_reply.started":"2024-06-30T16:32:26.482806Z","shell.execute_reply":"2024-06-30T16:32:29.569292Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":">>> Downloading ollama...\n######################################################################## 100.0%#=#=#                                                                          \n>>> Installing ollama to /usr/local/bin...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}]},{"cell_type":"code","source":"#un thread demone avvia il server locale di ollama\nimport subprocess\nimport threading\nt = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()","metadata":{"id":"c43LDFSFUqfY","execution":{"iopub.status.busy":"2024-06-30T16:32:29.572389Z","iopub.execute_input":"2024-06-30T16:32:29.572668Z","iopub.status.idle":"2024-06-30T16:32:29.579173Z","shell.execute_reply.started":"2024-06-30T16:32:29.572641Z","shell.execute_reply":"2024-06-30T16:32:29.578230Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"#un altro thread demone avvia llama3\n!ollama pull llama3\nt2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MQMgyp9U0hL","outputId":"fd6addf1-d873-498d-f0e6-8dea75381e7e","scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T16:32:29.580397Z","iopub.execute_input":"2024-06-30T16:32:29.581187Z","iopub.status.idle":"2024-06-30T16:32:35.192112Z","shell.execute_reply.started":"2024-06-30T16:32:29.581152Z","shell.execute_reply":"2024-06-30T16:32:35.191042Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stderr","text":"2024/06/30 16:32:29 routes.go:1064: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-06-30T16:32:29.596Z level=INFO source=images.go:730 msg=\"total blobs: 5\"\ntime=2024-06-30T16:32:29.596Z level=INFO source=images.go:737 msg=\"total unused blobs removed: 0\"\ntime=2024-06-30T16:32:29.597Z level=INFO source=routes.go:1111 msg=\"Listening on 127.0.0.1:11434 (version 0.1.48)\"\ntime=2024-06-30T16:32:29.598Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama2690156302/runners\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/30 - 16:32:34 | 200 |      71.386µs |       127.0.0.1 | HEAD     \"/\"\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-30T16:32:34.246Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu_avx2 cuda_v11 rocm_v60101 cpu cpu_avx]\"\ntime=2024-06-30T16:32:34.365Z level=INFO source=types.go:98 msg=\"inference compute\" id=GPU-b8c1fbd7-a663-db05-3763-9ec20ff0e227 library=cuda compute=6.0 driver=12.2 name=\"Tesla P100-PCIE-16GB\" total=\"15.9 GiB\" available=\"15.6 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h[GIN] 2024/06/30 - 16:32:35 | 200 |  717.975196ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \npulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \npulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \npulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \npulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \npulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \u001b[?25h\n","output_type":"stream"}]},{"cell_type":"code","source":"moderator_instructions = [\n    \"You are a moderator. Given the following rules, please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\",\n    \"You are a owner of the Social. Given the following rules,please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\",\n    \"You are a dictator moderator. Given the following rules, please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\"\n]\n\nnum_profili = 4\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def ask_to_llama(instruction,rules, prompt):    \n    full_context = instruction\n    for i, rule in enumerate(rules, start=1):\n        full_context += f\"{i}. {rule['text']}\\n\"\n    \n    full_prompt = full_context + prompt\n    \n    response = requests.post('http://localhost:11434/api/generate', \n                             data=json.dumps({'model': 'llama3', 'prompt': full_prompt, 'stream': False}), \n                             headers={'Content-Type': 'application/json'})\n    \n    return response.json()['response']","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:32:35.196201Z","iopub.execute_input":"2024-06-30T16:32:35.196589Z","iopub.status.idle":"2024-06-30T16:32:35.204340Z","shell.execute_reply.started":"2024-06-30T16:32:35.196551Z","shell.execute_reply":"2024-06-30T16:32:35.203379Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"def process_json_files(instances):\n    \n    start_time = time.time()\n    instanze = os.listdir(instances)\n    \n    \n    for filename in instanze:\n        if filename.endswith('.json'):\n            filepath = os.path.join(instances, filename)\n            with open(filepath, 'r') as file:\n                data = json.load(file)\n                rules = data['rules']\n                records = data['records']\n                records_en=[]\n                \n                for record in records:\n                    lan = record['language']\n                    if lan==\"en\":\n                        records_en.append(record)\n                        \n                if len(records_en)==0:\n                    print(f\"Per l'istanza {data['name']} non ci sono post in inglese\")\n                    filt_recorods = []\n                    \n                if len(records_en)>200: #se len>200 prendiamo solo testa e coda\n                    records_ordinati = sorted(records_en, key=lambda x: x['favourites'])\n                    primi_100 = records_ordinati[:100]\n                    ultimi_100 = records_ordinati[-100:]\n                    filt_records = primi_100 + ultimi_100\n                    \n                else: #altrimenti li prendiamo tutti\n                    filt_records = records_en\n                \n                #i=0\n                for record in filt_records:\n                    record_text = record['text']\n                    #i+=1\n                    for instruction in moderator_instructions:\n                        response = ask_to_llama(instruction,rules,record_text)\n                        print(f\"Response for record ID {record['id']} with instruction '{instruction.split()[3]}': {response}\")\n                        '''if (i%100 == 0):\n                            tempo = time.time() - start_time\n                            print(f\"PROCESSATE {i} in tempo: \",tempo)'''\n                        #break #così vediamo cosa fa con le altre instanze\n                    #break \n        #break\n    end_time = time.time() \n    duration = end_time - start_time\n    print(\"DURATA: \", duration)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T16:36:39.692358Z","iopub.execute_input":"2024-06-30T16:36:39.692759Z","iopub.status.idle":"2024-06-30T16:36:39.703877Z","shell.execute_reply.started":"2024-06-30T16:36:39.692729Z","shell.execute_reply":"2024-06-30T16:36:39.703014Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"instances = '/kaggle/input/instance-json/results'\nprocess_json_files(instances)\n#len(os.listdir(instances))//num_profili\n","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T16:40:53.102276Z","iopub.execute_input":"2024-06-30T16:40:53.103109Z","iopub.status.idle":"2024-06-30T16:40:53.111333Z","shell.execute_reply.started":"2024-06-30T16:40:53.103065Z","shell.execute_reply":"2024-06-30T16:40:53.110370Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"197"},"metadata":{}}]},{"cell_type":"code","source":"pip install bertopic","metadata":{},"execution_count":null,"outputs":[]}]}