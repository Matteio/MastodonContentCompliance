{"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyML1DjynvUMn9A08Tx/vxFB","include_colab_link":true},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8819968,"sourceType":"datasetVersion","datasetId":5306056}],"dockerImageVersionId":30733,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a href=\"https://colab.research.google.com/github/NicolaGabriele/MastodonContentCompliance/blob/main/post_ollama_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>","metadata":{"id":"view-in-github","colab_type":"text"}},{"cell_type":"code","source":"import requests\nimport json\nimport os\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-06-30T12:25:59.515098Z","iopub.execute_input":"2024-06-30T12:25:59.515811Z","iopub.status.idle":"2024-06-30T12:25:59.519632Z","shell.execute_reply.started":"2024-06-30T12:25:59.515779Z","shell.execute_reply":"2024-06-30T12:25:59.518762Z"},"trusted":true},"execution_count":64,"outputs":[]},{"cell_type":"code","source":"#istallazione di ollama\n!curl -fsSL https://ollama.com/install.sh | sh","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wkDuXLuzUk1K","outputId":"09da6314-1017-4217-c391-6ee81bdeca7d","scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T12:58:44.965338Z","iopub.execute_input":"2024-06-30T12:58:44.965691Z","iopub.status.idle":"2024-06-30T12:58:48.651273Z","shell.execute_reply.started":"2024-06-30T12:58:44.965662Z","shell.execute_reply":"2024-06-30T12:58:48.650377Z"},"trusted":true},"execution_count":103,"outputs":[{"name":"stdout","text":">>> Downloading ollama...\n######################################################################## 100.0%\n>>> Installing ollama to /usr/local/bin...\n>>> Adding ollama user to video group...\n>>> Adding current user to ollama group...\n>>> Creating ollama systemd service...\n>>> NVIDIA GPU installed.\n>>> The Ollama API is now available at 127.0.0.1:11434.\n>>> Install complete. Run \"ollama\" from the command line.\n","output_type":"stream"}]},{"cell_type":"code","source":"#un thread demone avvia il server locale di ollama\nimport subprocess\nimport threading\nt = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"serve\"]),daemon=True)\nt.start()","metadata":{"id":"c43LDFSFUqfY","execution":{"iopub.status.busy":"2024-06-30T12:58:48.653096Z","iopub.execute_input":"2024-06-30T12:58:48.653440Z","iopub.status.idle":"2024-06-30T12:58:48.660302Z","shell.execute_reply.started":"2024-06-30T12:58:48.653408Z","shell.execute_reply":"2024-06-30T12:58:48.659218Z"},"trusted":true},"execution_count":104,"outputs":[]},{"cell_type":"code","source":"#un altro thread demone avvia llama3\n!ollama pull llama3\nt2 = threading.Thread(target=lambda: subprocess.run([\"ollama\", \"run\", \"llama3\"]),daemon=True)\nt2.start()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9MQMgyp9U0hL","outputId":"fd6addf1-d873-498d-f0e6-8dea75381e7e","scrolled":true,"execution":{"iopub.status.busy":"2024-06-30T12:58:48.662047Z","iopub.execute_input":"2024-06-30T12:58:48.662682Z","iopub.status.idle":"2024-06-30T12:58:54.851142Z","shell.execute_reply.started":"2024-06-30T12:58:48.662651Z","shell.execute_reply":"2024-06-30T12:58:54.850140Z"},"trusted":true},"execution_count":105,"outputs":[{"name":"stderr","text":"2024/06/30 12:58:48 routes.go:1064: INFO server config env=\"map[CUDA_VISIBLE_DEVICES: GPU_DEVICE_ORDINAL: HIP_VISIBLE_DEVICES: HSA_OVERRIDE_GFX_VERSION: OLLAMA_DEBUG:false OLLAMA_FLASH_ATTENTION:false OLLAMA_HOST:http://127.0.0.1:11434 OLLAMA_INTEL_GPU:false OLLAMA_KEEP_ALIVE: OLLAMA_LLM_LIBRARY: OLLAMA_MAX_LOADED_MODELS:1 OLLAMA_MAX_QUEUE:512 OLLAMA_MAX_VRAM:0 OLLAMA_MODELS:/root/.ollama/models OLLAMA_NOHISTORY:false OLLAMA_NOPRUNE:false OLLAMA_NUM_PARALLEL:1 OLLAMA_ORIGINS:[http://localhost https://localhost http://localhost:* https://localhost:* http://127.0.0.1 https://127.0.0.1 http://127.0.0.1:* https://127.0.0.1:* http://0.0.0.0 https://0.0.0.0 http://0.0.0.0:* https://0.0.0.0:* app://* file://* tauri://*] OLLAMA_RUNNERS_DIR: OLLAMA_SCHED_SPREAD:false OLLAMA_TMPDIR: ROCR_VISIBLE_DEVICES:]\"\ntime=2024-06-30T12:58:48.674Z level=INFO source=images.go:730 msg=\"total blobs: 5\"\ntime=2024-06-30T12:58:48.674Z level=INFO source=images.go:737 msg=\"total unused blobs removed: 0\"\ntime=2024-06-30T12:58:48.675Z level=INFO source=routes.go:1111 msg=\"Listening on 127.0.0.1:11434 (version 0.1.48)\"\ntime=2024-06-30T12:58:48.676Z level=INFO source=payload.go:30 msg=\"extracting embedded files\" dir=/tmp/ollama3022589128/runners\n","output_type":"stream"},{"name":"stdout","text":"[GIN] 2024/06/30 - 12:58:53 | 200 |      53.788µs |       127.0.0.1 | HEAD     \"/\"\n","output_type":"stream"},{"name":"stderr","text":"time=2024-06-30T12:58:53.655Z level=INFO source=payload.go:44 msg=\"Dynamic LLM libraries [cpu cpu_avx cpu_avx2 cuda_v11 rocm_v60101]\"\ntime=2024-06-30T12:58:53.773Z level=INFO source=types.go:98 msg=\"inference compute\" id=GPU-7ea08e54-55f5-0453-4813-b05fa6628449 library=cuda compute=6.0 driver=12.2 name=\"Tesla P100-PCIE-16GB\" total=\"15.9 GiB\" available=\"15.6 GiB\"\n","output_type":"stream"},{"name":"stdout","text":"\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠴ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠦ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠧ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠇ \u001b[?25h[GIN] 2024/06/30 - 12:58:54 | 200 |  969.241606ms |       127.0.0.1 | POST     \"/api/pull\"\n\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \npulling 6a0746a1ec1a... 100% ▕████████████████▏ 4.7 GB                         \npulling 4fa551d4f938... 100% ▕████████████████▏  12 KB                         \npulling 8ab4849b038c... 100% ▕████████████████▏  254 B                         \npulling 577073ffcc6c... 100% ▕████████████████▏  110 B                         \npulling 3f8eb4da87fa... 100% ▕████████████████▏  485 B                         \nverifying sha256 digest \nwriting manifest \nremoving any unused layers \nsuccess \u001b[?25h\n","output_type":"stream"}]},{"cell_type":"code","source":"moderator_instructions = [\n    \"You are a moderator. Given the following rules, please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\",\n    \"You are a owner of the Social. Given the following rules,please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\",\n    \"You are a dictator moderator. Given the following rules, please provide a score from 1 to 10 based on how well the rules are respected, and don't give a brief explanation of your score:\\n\"\n]\n\n\ndef ask_to_llama(instruction,rules, prompt):    \n    full_context = instruction\n    for i, rule in enumerate(rules, start=1):\n        full_context += f\"{i}. {rule['text']}\\n\"\n    \n    full_prompt = full_context + prompt\n    \n    response = requests.post('http://localhost:11434/api/generate', \n                             data=json.dumps({'model': 'llama3', 'prompt': full_prompt, 'stream': False}), \n                             headers={'Content-Type': 'application/json'})\n    \n    return response.json()['response']","metadata":{"execution":{"iopub.status.busy":"2024-06-30T12:58:54.854403Z","iopub.execute_input":"2024-06-30T12:58:54.854773Z","iopub.status.idle":"2024-06-30T12:58:54.863215Z","shell.execute_reply.started":"2024-06-30T12:58:54.854716Z","shell.execute_reply":"2024-06-30T12:58:54.861780Z"},"trusted":true},"execution_count":106,"outputs":[]},{"cell_type":"code","source":"def process_json_files(instances):\n    start_time = time.time()\n    for filename in os.listdir(instances):\n        if filename.endswith('.json'):\n            filepath = os.path.join(instances, filename)\n            with open(filepath, 'r') as file:\n                data = json.load(file)\n                \n                rules = data['rules']\n                records = data['records']\n                \n                records_ordinati = sorted(records, key=lambda x: x['favourites'])\n                primi_100 = records_ordinati[:100]\n                ultimi_100 = records_ordinati[-100:]\n                \n                filt_records = primi_100 + ultimi_100\n                \n                i=0\n                \n                for record in filt_records:\n                    record_text = record['text']\n                    i+=1\n                    for instruction in moderator_instructions:\n                        response = ask_to_llama(instruction,rules,record_text)\n                        print(f\"Response for record ID {record['id']} with instruction '{instruction.split()[3]}': {response}\")\n                        if (i%100 == 0):\n                            tempo = time.time() - start_time\n                            print(f\"PROCESSATE {i} in tempo: \",tempo)\n        break\n    end_time = time.time() \n    duration = end_time - start_time\n    print(\"DURATA: \", duration)","metadata":{"execution":{"iopub.status.busy":"2024-06-30T13:19:37.985882Z","iopub.execute_input":"2024-06-30T13:19:37.986263Z","iopub.status.idle":"2024-06-30T13:19:37.994927Z","shell.execute_reply.started":"2024-06-30T13:19:37.986234Z","shell.execute_reply":"2024-06-30T13:19:37.993912Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"instances = '/kaggle/input/instance-json/results'\nprocess_json_files(instances)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}